import transformers
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Use the model you want
transformers_model = "obi/deid_roberta_i2b2"

# Optional: specify a custom cache directory (in current folder, so no admin issues)
local_dir = "./hf_model_cache"

# Download model snapshot locally
local_model_path = snapshot_download(repo_id=transformers_model, cache_dir=local_dir)

# Force load to ensure it's cached and won't download again at runtime
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForTokenClassification.from_pretrained(local_model_path)




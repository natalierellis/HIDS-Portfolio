import os
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForTokenClassification

# --- Offline mode ---
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

# --- Model info ---
transformers_model = "obi/deid_roberta_i2b2"
local_dir = "./hf_model_cache"

# --- Determine snapshot path ---
snapshot_subdir = os.path.join(local_dir, f"models--{transformers_model.replace('/', '--')}")
snapshot_root = os.path.join(snapshot_subdir, "snapshots")

# --- If not already cached, download it ---
if not os.path.exists(snapshot_root) or not os.listdir(snapshot_root):
    print("Model not found locally. Downloading...")
    local_model_path = snapshot_download(repo_id=transformers_model, cache_dir=local_dir)
else:
    print("Model already downloaded. Using local copy.")
    snapshots = os.listdir(snapshot_root)
    if snapshots:
        snapshot_folder = snapshots[0]
        local_model_path = os.path.join(snapshot_root, snapshot_folder)
    else:
        raise ValueError("Snapshot folder found, but no snapshot contents available.")

# âœ… This is what you're referring to:
# Use `local_model_path` to load both the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForTokenClassification.from_pretrained(local_model_path)

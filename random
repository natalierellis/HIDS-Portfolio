**Slide 1: Weekly NLP Progress Update**

**Speaker Notes:**
This week I focused on implementing and refining a natural language processing (NLP) solution for de-identifying clinical text in our REDCap exports. I evaluated several tools, ran test cases, addressed model performance issues, and made significant speed improvements to support full dataset processing.

---

**Slide 2: Tool Evaluation & Setup**

**Key Points:**

* Evaluated SpaCy, SciSpaCy, and Microsoft Presidio
* Discovered Presidio runs on SpaCy or other NER models under the hood
* Prioritized Presidio for flexibility and integration

**Speaker Notes:**
I started by researching different NLP tools. Although they appear separate, SpaCy is actually used by Presidio internally. I selected Presidio because it's modular and customizable, allowing us to plug in a clinical NER model.

---

**Slide 3: Dummy Dataset Testing**

**Key Points:**

* Ran Presidio on dummy data stored in Excel
* Goal: Verify it works offline and ensure no data leaves my machine
* Model worked but lacked medical context

**Speaker Notes:**
Before applying it to real patient data, I tested the setup on dummy text within an Excel file. This helped me ensure nothing was connected to the internet and gave me a better understanding of how the system operates. However, the default model wasn’t trained on medical data and falsely flagged clinical terms as names.

---

**Slide 4: Clinical Model Integration**

**Key Points:**

* Default model flagged medical terms as names (e.g., "HG serous carcinoma")
* Searched for better clinical models trained on EHR notes
* Integrated the HuggingFace model `obi/deid_roberta_i2b2` into Presidio

**Speaker Notes:**
To improve accuracy, I spent Monday identifying and integrating a better model trained on de-identifying clinical notes. This model understands terms like "HG serous carcinoma" and dramatically reduces false positives.

---

**Slide 5: Pilot Test on Real REDCap Subset**

**Key Points:**

* Ran new model on "summary for final TDAN diagnosis" column
* 100 patients, \~3 minutes processing time
* Confirmed performance was significantly better

**Speaker Notes:**
After switching models, I tested the new setup on a real REDCap column. The runtime was manageable and the false positives dropped substantially, especially on common clinical terms.

---

**Slide 6: Full Dataset & Performance Issues**

**Key Points:**

* Processed 34 columns, 660 patients (Duke 2012+)
* Took 1.5 hours
* Identified performance issues due to blank & non-text fields

**Speaker Notes:**
Taylor and I created a batch report for the PHI fields. Running the model on the full dataset took too long, so I investigated the cause. Many blank or irrelevant fields like "Amended By" were slowing down the process.

---

**Slide 7: Code Optimization**

**Key Points:**

* Skipped blanks and irrelevant fields (e.g., amended by, dropdowns)
* Reduced runtime from 1.5 hours to \~20-30 minutes
* Future strategy: specify relevant fields ahead of time

**Speaker Notes:**
I added logic to skip over empty cells and fields that don’t require NLP. This greatly improved performance and gave us a clear plan for targeting only relevant text fields in future analyses.

---

**Slide 8: Collaborations & Next Steps**

**Key Points:**

* Discussed computing speed with other devs
* Plan to explore running model on VM/server with Jonathan
* Working to separate patient vs staff vs provider labels

**Speaker Notes:**
I spoke with other developers about slow processing and learned we might be able to run this faster on a VM or dedicated server. I'm also working to modify the model to distinguish between patient names and provider/staff names.

---

**Slide 9: Entity Differentiation Challenges**

**Key Points:**

* Model outputs general "PERSON" label for all name types
* Working to map back to original tags (STAFF vs PATIENT)
* Interim solution: manual filtering (e.g., exclude "last TDAN FU")

**Speaker Notes:**
Right now, the model treats everyone as a generic "PERSON." I’m investigating whether we can extract the original labels like "STAFF" or "PATIENT." Meanwhile, I’m writing logic to catch false positives, like excluding labels that follow "Dr." or match known non-names.

---

**Slide 10: Preliminary Findings**

**Key Points:**

* Identified: \[insert number] "PERSON" entities
* Staff names detected: \[insert number], Patient names: \[insert number]
* Accuracy (after filtering): \[insert %] confirmed as true PHI

**Speaker Notes:**
Finally, here are our early results. The system flagged \[X] people. I manually validated a subset and found that \[X%] were valid PHI entities. As we improve filtering and labeling, accuracy will continue to rise.

---

**Slide 11: Summary**

**Key Accomplishments:**

* Integrated Presidio with a clinical-grade model
* Successfully processed real REDCap text fields
* Reduced runtime by optimizing for blanks and irrelevant fields
* Working toward accurate staff vs patient differentiation

**Speaker Notes:**
To summarize, we now have a working de-identification pipeline for REDCap text fields that respects clinical context. Performance has improved significantly, and we’re actively working to fine-tune entity classification and scale this across the broader dataset.
